#! /usr/bin/env python
from __future__ import unicode_literals, division, print_function
import os, sys, functools, json
import numpy as np
from scipy.sparse import csr_matrix
from six import iterkeys, iteritems

from sli.parser import parse_line, parse_args_str, parse_option_args, parse_fd_args
from sli.processing import FreqCounter, inspect_line, log_pipeline, lines_from_file, \
    remove_events, opt_arg_features, fd_features

# Option arguments
_OPT_ARGS_MAP = {
    "mmap": ["prot", "flags"],
    "futex": ["op"],
    "access": ["mode"],
    "open": ["flags"],
    "fcntl": ["cmd"],
    "lseek": ["whence"],
    "clone": ["flags"]
}

# File descriptor path patterns
_FD_PATH_PATTERNS = [
    # Top-level directories
    r"^\/bin",
    r"^\/dev",
    r"^\/etc",
    r"^\/home",
    r"^\/lib",
    r"^\/proc",
    r"^\/run",
    r"^\/sbin",
    r"^\/sys",
    r"^\/tmp",
    r"^\/usr",
    r"^\/var"
]

def run_shared_pipeline(file_path, freq_counter, training=False):
    """ Run shared pipeline for training and testing. """
    return log_pipeline(
        lines_from_file(file_path),
        # Parse line
        parse_line(),
        # Remove given events
        remove_events("switch"),
        # Parse arguments string
        parse_args_str(),
        # Parse option arguments
        parse_option_args(event_opt_args=_OPT_ARGS_MAP),
        # Parse file descriptor arguments
        parse_fd_args(),
        # Process lines with frequency counter
        freq_counter.process_lines(training=training)
    )

def log_path(dataset_root, dataset_name, i):
    """ Helper function for assembling log file path. """
    return os.path.join(
        dataset_root,
        dataset_name,
        "{}log-{}.txt".format(dataset_name, i)
    )

def process_dataset_part(dataset_root, idx_map, freq_counter, training=False):
    """ Process part of the dataset. """
    # Temporary result and labels
    x_tmp = []
    labels = []
    # Dataset names
    dataset_names = sorted(iterkeys(idx_map))
    # Process all training set
    for label, dataset_name in enumerate(dataset_names):
        idx_array = idx_map[dataset_name]
        idx_size = len(idx_array)
        # Process logs
        for i, idx in enumerate(idx_array):
            print("Processing {} log #{} ({}/{})".format(dataset_name, idx+1, i+1, idx_size))
            # Run pipeline for each log
            x_tmp += run_shared_pipeline(
                log_path(dataset_root, dataset_name, idx+1),
                freq_counter,
                training
            )
        # Append labels
        labels = np.concatenate([labels, np.repeat(label, idx_size)])
    # Count frequency on each log
    x = log_pipeline(x_tmp, freq_counter.count_freq)
    # Reshape matrix into vector
    x = np.reshape(x, (len(x), -1))
    return x, labels

def process_dataset(dataset_root, dataset_size_map):
    """ Process whole dataset and generate training, validation and testing data. """
    # Frequency counter
    freq_counter = FreqCounter(feature_generators=[
        opt_arg_features(_OPT_ARGS_MAP),
        fd_features(_FD_PATH_PATTERNS)
    ])
    # Training, validation and testing set indexes
    train_idx_map = {}
    validate_idx_map = {}
    test_idx_map = {}
    # Dataset names
    dataset_names = sorted(iterkeys(dataset_size_map))
    # Generate indexes
    for dataset_name, size in iteritems(dataset_size_map):
        rand_idx = np.random.permutation(size)
        # Training, validation and test set range
        train_max = int(size*0.6)
        validate_max = train_max+int(size*0.2)
        # Set indexes
        train_idx_map[dataset_name] = rand_idx[:train_max]
        validate_idx_map[dataset_name] = rand_idx[train_max:validate_max]
        test_idx_map[dataset_name] = rand_idx[validate_max:]
    # Training, validation and testing data
    print("Processing training set")
    x_train, labels_train = process_dataset_part(
        dataset_root, train_idx_map, freq_counter, True
    )
    print("Processing validation set")
    x_validate, labels_validate = process_dataset_part(
        dataset_root, validate_idx_map, freq_counter
    )
    print("Processing testing set")
    x_test, labels_test = process_dataset_part(
        dataset_root, test_idx_map, freq_counter
    )
    # Compress feature data for smaller file size
    return csr_matrix(x_train), \
        labels_train, \
        csr_matrix(x_validate), \
        labels_validate, \
        csr_matrix(x_test), \
        labels_test, \
        dataset_names, \
        sorted(freq_counter.processes), \
        sorted(freq_counter.evt_opt_tuples)

if __name__=="__main__":
    # Prompt usage
    if len(sys.argv)<4:
        print("Usage: {} [Dataset Path] [Dataset Size Map] [Output File]".format(sys.argv[0]))
        exit(1)
    # Load dataset information
    with open(sys.argv[2]) as f:
        dataset_size_map = json.load(f)
    # Process dataset and write features
    process_result = process_dataset(sys.argv[1], dataset_size_map)
    with open(sys.argv[3], "wb") as f:
        np.save(f, process_result)
